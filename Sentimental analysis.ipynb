{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9fba34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "872d3636",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "635fe0b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reviewer Name</th>\n",
       "      <th>Review Title</th>\n",
       "      <th>Place of Review</th>\n",
       "      <th>Up Votes</th>\n",
       "      <th>Down Votes</th>\n",
       "      <th>Month</th>\n",
       "      <th>Review text</th>\n",
       "      <th>Ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kamal Suresh</td>\n",
       "      <td>Nice product</td>\n",
       "      <td>Certified Buyer, Chirakkal</td>\n",
       "      <td>889.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>Feb 2021</td>\n",
       "      <td>Nice product, good quality, but price is now r...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Flipkart Customer</td>\n",
       "      <td>Don't waste your money</td>\n",
       "      <td>Certified Buyer, Hyderabad</td>\n",
       "      <td>109.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Feb 2021</td>\n",
       "      <td>They didn't supplied Yonex Mavis 350. Outside ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A. S. Raja Srinivasan</td>\n",
       "      <td>Did not meet expectations</td>\n",
       "      <td>Certified Buyer, Dharmapuri</td>\n",
       "      <td>42.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Apr 2021</td>\n",
       "      <td>Worst product. Damaged shuttlecocks packed in ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Suresh Narayanasamy</td>\n",
       "      <td>Fair</td>\n",
       "      <td>Certified Buyer, Chennai</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Quite O. K. , but nowadays  the quality of the...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ASHIK P A</td>\n",
       "      <td>Over priced</td>\n",
       "      <td>NaN</td>\n",
       "      <td>147.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>Apr 2016</td>\n",
       "      <td>Over pricedJust â?¹620 ..from retailer.I didn'...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8513</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8514</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8515</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8516</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8517</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8518 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Reviewer Name               Review Title  \\\n",
       "0               Kamal Suresh               Nice product   \n",
       "1          Flipkart Customer     Don't waste your money   \n",
       "2     A. S. Raja Srinivasan   Did not meet expectations   \n",
       "3        Suresh Narayanasamy                       Fair   \n",
       "4                  ASHIK P A                Over priced   \n",
       "...                      ...                        ...   \n",
       "8513                     NaN                        NaN   \n",
       "8514                     NaN                        NaN   \n",
       "8515                     NaN                        NaN   \n",
       "8516                     NaN                        NaN   \n",
       "8517                     NaN                        NaN   \n",
       "\n",
       "                  Place of Review  Up Votes  Down Votes     Month  \\\n",
       "0      Certified Buyer, Chirakkal     889.0        64.0  Feb 2021   \n",
       "1      Certified Buyer, Hyderabad     109.0         6.0  Feb 2021   \n",
       "2     Certified Buyer, Dharmapuri      42.0         3.0  Apr 2021   \n",
       "3        Certified Buyer, Chennai      25.0         1.0       NaN   \n",
       "4                             NaN     147.0        24.0  Apr 2016   \n",
       "...                           ...       ...         ...       ...   \n",
       "8513                          NaN       NaN         NaN       NaN   \n",
       "8514                          NaN       NaN         NaN       NaN   \n",
       "8515                          NaN       NaN         NaN       NaN   \n",
       "8516                          NaN       NaN         NaN       NaN   \n",
       "8517                          NaN       NaN         NaN       NaN   \n",
       "\n",
       "                                            Review text  Ratings  \n",
       "0     Nice product, good quality, but price is now r...        4  \n",
       "1     They didn't supplied Yonex Mavis 350. Outside ...        1  \n",
       "2     Worst product. Damaged shuttlecocks packed in ...        1  \n",
       "3     Quite O. K. , but nowadays  the quality of the...        3  \n",
       "4     Over pricedJust â?¹620 ..from retailer.I didn'...        1  \n",
       "...                                                 ...      ...  \n",
       "8513                                                NaN        5  \n",
       "8514                                                NaN        2  \n",
       "8515                                                NaN        4  \n",
       "8516                                                NaN        1  \n",
       "8517                                                NaN        4  \n",
       "\n",
       "[8518 rows x 8 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bd5435e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8518 entries, 0 to 8517\n",
      "Data columns (total 8 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   Reviewer Name    8508 non-null   object \n",
      " 1   Review Title     8508 non-null   object \n",
      " 2   Place of Review  8468 non-null   object \n",
      " 3   Up Votes         8508 non-null   float64\n",
      " 4   Down Votes       8508 non-null   float64\n",
      " 5   Month            8053 non-null   object \n",
      " 6   Review text      8510 non-null   object \n",
      " 7   Ratings          8518 non-null   int64  \n",
      "dtypes: float64(2), int64(1), object(5)\n",
      "memory usage: 532.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fb346e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['Review text'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "376882dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 8510 entries, 0 to 8509\n",
      "Data columns (total 8 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   Reviewer Name    8508 non-null   object \n",
      " 1   Review Title     8508 non-null   object \n",
      " 2   Place of Review  8468 non-null   object \n",
      " 3   Up Votes         8508 non-null   float64\n",
      " 4   Down Votes       8508 non-null   float64\n",
      " 5   Month            8053 non-null   object \n",
      " 6   Review text      8510 non-null   object \n",
      " 7   Ratings          8510 non-null   int64  \n",
      "dtypes: float64(2), int64(1), object(5)\n",
      "memory usage: 598.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ca6039",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c7a9673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\jsnat\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: click in c:\\users\\jsnat\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\jsnat\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\jsnat\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\jsnat\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\jsnat\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7d89b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\jsnat\\anaconda3\\lib\\site-packages (4.3.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\jsnat\\anaconda3\\lib\\site-packages (from gensim) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\jsnat\\anaconda3\\lib\\site-packages (from gensim) (1.10.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\jsnat\\anaconda3\\lib\\site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: FuzzyTM>=0.4.0 in c:\\users\\jsnat\\anaconda3\\lib\\site-packages (from gensim) (2.0.5)\n",
      "Requirement already satisfied: pandas in c:\\users\\jsnat\\anaconda3\\lib\\site-packages (from FuzzyTM>=0.4.0->gensim) (1.5.3)\n",
      "Requirement already satisfied: pyfume in c:\\users\\jsnat\\anaconda3\\lib\\site-packages (from FuzzyTM>=0.4.0->gensim) (0.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\jsnat\\anaconda3\\lib\\site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\jsnat\\anaconda3\\lib\\site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2022.7)\n",
      "Requirement already satisfied: simpful in c:\\users\\jsnat\\anaconda3\\lib\\site-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (2.12.0)\n",
      "Requirement already satisfied: fst-pso in c:\\users\\jsnat\\anaconda3\\lib\\site-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (1.8.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\jsnat\\anaconda3\\lib\\site-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (4.6.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jsnat\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->FuzzyTM>=0.4.0->gensim) (1.16.0)\n",
      "Requirement already satisfied: miniful in c:\\users\\jsnat\\anaconda3\\lib\\site-packages (from fst-pso->pyfume->FuzzyTM>=0.4.0->gensim) (0.0.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57edd94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jsnat\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\jsnat\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jsnat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jsnat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jsnat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\jsnat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Load BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "# Check for missing values in 'Review text' column\n",
    "df.dropna(subset=['Review text'], inplace=True)\n",
    "\n",
    "# Text Cleaning\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):\n",
    "        # Remove special characters and punctuation\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        return text\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def remove_stopwords(text):\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Apply text cleaning to Review text column\n",
    "df['Cleaned Text'] = df['Review text'].apply(clean_text)\n",
    "df['Cleaned Text'] = df['Cleaned Text'].apply(remove_stopwords)\n",
    "\n",
    "# Text Normalization (Lemmatization)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_text(text):\n",
    "    words = word_tokenize(text)\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "# Apply lemmatization to cleaned text\n",
    "df['Normalized Text'] = df['Cleaned Text'].apply(lemmatize_text)\n",
    "\n",
    "# Numerical Feature Extraction\n",
    "# Bag-of-Words (BoW) Model\n",
    "bow_vectorizer = CountVectorizer()\n",
    "bow_features = bow_vectorizer.fit_transform(df['Normalized Text'])\n",
    "\n",
    "# Term Frequency-Inverse Document Frequency (TF-IDF) Model\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_features = tfidf_vectorizer.fit_transform(df['Normalized Text'])\n",
    "\n",
    "# Word2Vec Model\n",
    "word2vec_model = Word2Vec(sentences=[word_tokenize(text) for text in df['Normalized Text']], vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "def get_word_vector(word):\n",
    "    if word in word2vec_model.wv:\n",
    "        return word2vec_model.wv[word]\n",
    "    else:\n",
    "        return np.zeros(100)  # Return zero vector if word not found\n",
    "\n",
    "word2vec_features = np.array([\n",
    "    np.mean([get_word_vector(word) for word in word_tokenize(text)], axis=0)\n",
    "    for text in df['Normalized Text']\n",
    "])\n",
    "\n",
    "# BERT Model\n",
    "bert_tokenized_texts = df['Normalized Text'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True))\n",
    "bert_max_len = max(map(len, bert_tokenized_texts))\n",
    "bert_padded_texts = np.array([text + [0] * (bert_max_len - len(text)) for text in bert_tokenized_texts])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ef99c4",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28330ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score (Logistic Regression): 0.5591318931811269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jsnat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Assuming you have a target variable 'Sentiment' in your dataframe\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(bow_features, df['Ratings'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Logistic Regression Model\n",
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = lr_model.predict(X_test)\n",
    "\n",
    "f1_score_lr = f1_score(y_test, y_pred, average='weighted')\n",
    "print(\"F1 Score (Logistic Regression):\", f1_score_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76f1e6df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: joblib in c:\\users\\jsnat\\anaconda3\\lib\\site-packages (1.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93ec21ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lr_model.pkl']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the model to a file\n",
    "joblib.dump(lr_model, 'lr_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5ae09eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model and Predict\n",
    "\n",
    "lr = joblib.load('lr_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0eb4e69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 5, 5, ..., 5, 5, 5], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be7dfa8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vectorizer.pkl']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model to a file\n",
    "joblib.dump(bow_vectorizer, 'vectorizer.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd519a2f",
   "metadata": {},
   "source": [
    "### tfidf_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ead44460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score (Logistic Regression): 0.5331188030604535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jsnat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Assuming you have a target variable 'Sentiment' in your dataframe\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_features, df['Ratings'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Logistic Regression Model\n",
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = lr_model.predict(X_test)\n",
    "\n",
    "f1_score_lr = f1_score(y_test, y_pred, average='weighted')\n",
    "print(\"F1 Score (Logistic Regression):\", f1_score_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37aa4054",
   "metadata": {},
   "source": [
    "### Word2vec_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d1ec3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score (Logistic Regression): 0.44271369560236373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jsnat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have a target variable 'Sentiment' in your dataframe\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(word2vec_features, df['Ratings'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Logistic Regression Model\n",
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = lr_model.predict(X_test)\n",
    "\n",
    "f1_score_lr = f1_score(y_test, y_pred, average='weighted')\n",
    "print(\"F1 Score (Logistic Regression):\", f1_score_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5e4960",
   "metadata": {},
   "source": [
    "### SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b0f8310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score (SVM): 0.44271369560236373\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_model = SVC(kernel='linear')\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred_svm = svm_model.predict(X_test)\n",
    "f1_score_svm = f1_score(y_test, y_pred_svm,average='weighted')\n",
    "print(\"F1 Score (SVM):\", f1_score_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6eea3e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score (SVM): 0.44271369560236373\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_model = SVC(kernel='rbf')\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred_svm = svm_model.predict(X_test)\n",
    "f1_score_svm = f1_score(y_test, y_pred_svm,average='weighted')\n",
    "print(\"F1 Score (SVM):\", f1_score_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0018e31f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score (SVM): 0.40658260500730187\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_model = SVC(kernel='sigmoid')\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred_svm = svm_model.predict(X_test)\n",
    "f1_score_svm = f1_score(y_test, y_pred_svm,average='weighted')\n",
    "print(\"F1 Score (SVM):\", f1_score_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2d07ad",
   "metadata": {},
   "source": [
    "#### Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de7cb436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jsnat\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From C:\\Users\\jsnat\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\jsnat\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "213/213 [==============================] - 9s 26ms/step - loss: -61809936.0000 - accuracy: 0.0881 - val_loss: -570884800.0000 - val_accuracy: 0.0946\n",
      "Epoch 2/10\n",
      "213/213 [==============================] - 5s 25ms/step - loss: -26742798336.0000 - accuracy: 0.0889 - val_loss: -111304417280.0000 - val_accuracy: 0.0946\n",
      "Epoch 3/10\n",
      "213/213 [==============================] - 6s 26ms/step - loss: -672426950656.0000 - accuracy: 0.0889 - val_loss: -1788331556864.0000 - val_accuracy: 0.0946\n",
      "Epoch 4/10\n",
      "213/213 [==============================] - 5s 25ms/step - loss: -5418493411328.0000 - accuracy: 0.0889 - val_loss: -11233548828672.0000 - val_accuracy: 0.0946\n",
      "Epoch 5/10\n",
      "213/213 [==============================] - 5s 25ms/step - loss: -24859432189952.0000 - accuracy: 0.0889 - val_loss: -43872172376064.0000 - val_accuracy: 0.0946\n",
      "Epoch 6/10\n",
      "213/213 [==============================] - 5s 25ms/step - loss: -80838406438912.0000 - accuracy: 0.0889 - val_loss: -127605969780736.0000 - val_accuracy: 0.0946\n",
      "Epoch 7/10\n",
      "213/213 [==============================] - 6s 27ms/step - loss: -209015288102912.0000 - accuracy: 0.0889 - val_loss: -305057971044352.0000 - val_accuracy: 0.0946\n",
      "Epoch 8/10\n",
      "213/213 [==============================] - 6s 26ms/step - loss: -462502110429184.0000 - accuracy: 0.0889 - val_loss: -636106198482944.0000 - val_accuracy: 0.0946\n",
      "Epoch 9/10\n",
      "213/213 [==============================] - 5s 25ms/step - loss: -902934330081280.0000 - accuracy: 0.0889 - val_loss: -1196133225332736.0000 - val_accuracy: 0.0946\n",
      "Epoch 10/10\n",
      "213/213 [==============================] - 5s 24ms/step - loss: -1620854085517312.0000 - accuracy: 0.0889 - val_loss: -2080162719989760.0000 - val_accuracy: 0.0946\n",
      "54/54 [==============================] - 0s 5ms/step\n",
      "F1-Score: 0.016349683016349686\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(bow_features, df['Ratings'], test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_dense = X_train.toarray()\n",
    "X_test_dense = X_test.toarray()\n",
    "\n",
    "# Build the Deep Neural Network model\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(X_train_dense.shape[1],), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_dense, y_train, epochs=10, batch_size=32, validation_data=(X_test_dense, y_test))\n",
    "\n",
    "# Evaluate the model using F1-Score\n",
    "y_pred = model.predict(X_test_dense)\n",
    "y_pred_binary = [1 if pred > 0.5 else 0 for pred in y_pred]\n",
    "f1 = f1_score(y_test, y_pred_binary, average = 'weighted')\n",
    "print(\"F1-Score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e212fc7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Function `_wrapped_model` contains input name(s) resource with unsupported characters which will be renamed to sequential_dense_5_biasadd_readvariableop_resource in the SavedModel.\n",
      "INFO:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model.pkl\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model.pkl\\assets\n",
      "INFO:absl:Writing fingerprint to my_model.pkl\\fingerprint.pb\n"
     ]
    }
   ],
   "source": [
    "model.save(\"my_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9203c1a",
   "metadata": {},
   "source": [
    "## TEsting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67d980eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Cleaning\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):\n",
    "        # Remove special characters and punctuation\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        return text\n",
    "    else:\n",
    "        return ''\n",
    "    \n",
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def remove_stopwords(text):\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Text Normalization (Lemmatization)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_text(text):\n",
    "    words = word_tokenize(text)\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "# Apply text cleaning to Review text column\n",
    "def preprocess_text(review_text):\n",
    "    cleaned_text = clean_text(review_text)\n",
    "    cleaned_text = remove_stopwords(cleaned_text)\n",
    "    normalized_text = lemmatize_text(cleaned_text)\n",
    "    bow_features = bow_vectorizer.transform([normalized_text])\n",
    "    return bow_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "985f4c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "import joblib\n",
    "lr_model = joblib.load('lr_model.pkl')\n",
    "# Load the CountVectorizer object\n",
    "bow_vectorizer = joblib.load('vectorizer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "95e1d13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"They didn't supplied Yonex Mavis 350.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de151524",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_features = preprocess_text(input_text)\n",
    "predictions = lr_model.predict(bow_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "635b35f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5], dtype=int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b5f5e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
